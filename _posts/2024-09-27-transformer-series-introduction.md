---
layout: single
title: "Transformer Series: Introduction"
date: 2024-09-27 10:00:00 +0100
categories: 
  - deep-learning
tags:
  - Transformers
  - NLP
  - AI
---

![The Road to Transformers](/assets/images/images_posts/Transformer_series/the_road_to_transformers.png)

J'entame une série d'articles sur l'architecture des **transformers**. Aujourd'hui, ce paradigme dominant en **apprentissage profond** est omniprésent dans tous les aspects structurant notre rapport à la technologie et à l'intelligence artificielle. **Inventé en 2017** avec le papier *"Attention Is All You Need"*[^1], les transformers soutiennent les systèmes de recommandation des plateformes que nous utilisons au quotidien. Ils sont à la base des percées récentes en **traduction automatique** (comme **DeepL**) et révolutionnent même la recherche fondamentale en **biotechnologie**, avec des avancées telles qu'**AlphaFold**.

Il est donc absolument nécessaire de consacrer cette série d'articles à l'étude approfondie de cette architecture, toujours trop peu abordée dans les **cursus académiques en France**.

## Plan de la série

### 1. [Des RNN aux mécanismes d'attention](#) *(à venir)*
Dans ce premier article, nous reviendrons sur l'évolution des réseaux de neurones récurrents (RNN) vers les mécanismes d'attention qui forment la base des transformers.

### 2. [Attention Is All You Need](#) *(à venir)*
Nous plongerons dans l'article fondateur de 2017, en analysant ses idées révolutionnaires et la manière dont il a redéfini le domaine du **NLP** (Natural Language Processing).

### 3. [Les Transformers aujourd'hui et demain](#) *(à venir)*
Exploration des applications modernes des transformers, de **DeepL** à **GPT-2**, et des futurs développements dans des domaines comme la vision par ordinateur avec **ViT**.

### 4. [L'entraînement des Transformers](#) *(à venir)*
Un tutoriel pratique sur l'entraînement de votre propre modèle transformer à partir de zéro.

### 5. [Deux variantes populaires : BERT et GPT](#) *(à venir)*
Focus sur deux des modèles les plus influents issus de l'architecture transformer : **BERT** (Bidirectional Encoder Representations from Transformers) et **GPT** (Generative Pre-trained Transformer).

### 6. [Quelques aspects de l'algorithme d'AlphaFold](#) *(à venir)*
Discussion sur l'impact d'**AlphaFold** dans la biotechnologie et comment les transformers ont permis de résoudre des problèmes complexes dans la prédiction des structures protéiques.

## Notes de bas de page

[^1]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). *Attention is all you need.* In Advances in Neural Information Processing Systems (pp. 5998-6008).